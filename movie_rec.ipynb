{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "import pickle\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr_torch.inputs import SparseFeat, VarLenSparseFeat, get_feature_names\n",
    "from deepctr_torch.models.din import DIN\n",
    "from deepctr_torch.models import DeepFM\n",
    "import flwr as fl\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "from flwr.common import (\n",
    "    Code,\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Status,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCFG:\n",
    "    data_root = 'ml-1m'\n",
    "\n",
    "\n",
    "class FedCFG:\n",
    "    num_clients = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取训练数据以及辅助函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    key2index = {}\n",
    "    key_ans = x.split('|')\n",
    "    for key in key_ans:\n",
    "        if key not in key2index:\n",
    "            # Notice : input value 0 is a special \"padding\",so we do not use 0 to encode valid feature for sequence input\n",
    "            key2index[key] = len(key2index) + 1\n",
    "    # return torch.tensor(list(map(lambda x: key2index[x], key_ans)))\n",
    "    return torch.tensor([1, 2])\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv(os.path.join(\n",
    "        DatasetCFG.data_root, 'ratings_data_process.csv'))\n",
    "    sparse_features = [\"movieId\", \"userId\",\n",
    "                       \"gender\", \"age\", \"occupation\", \"zipCode\"]\n",
    "    target = ['rating']\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    # data['rating'] = data['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "    key2index = {}\n",
    "    genres_list = list(map(split, data['genres'].values))\n",
    "    genres_length = np.array(list(map(len, genres_list)))\n",
    "    max_len = max(genres_length)\n",
    "    # Notice : padding=`post`\n",
    "    genres_list = pad_sequence(genres_list)\n",
    "\n",
    "    # 2.count #unique features for each sparse field\n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(), embedding_dim=4)\n",
    "                              for feat in sparse_features]\n",
    "\n",
    "    varlen_feature_columns = [VarLenSparseFeat(SparseFeat('genres', vocabulary_size=len(\n",
    "        key2index) + 1, embedding_dim=4), maxlen=max_len, combiner='mean')]  # Notice : value 0 is for padding for sequence input feature\n",
    "\n",
    "    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "    data[\"genres\"] = genres_list.T\n",
    "\n",
    "    return data, feature_names, linear_feature_columns, dnn_feature_columns,  target\n",
    "\n",
    "\n",
    "def get_datas():\n",
    "    data = pd.read_csv(os.path.join(\n",
    "        DatasetCFG.data_root, 'ratings_data_process.csv'))\n",
    "    sparse_features = [\"movieId\", \"userId\",\n",
    "                       \"gender\", \"age\", \"occupation\", \"zipCode\"]\n",
    "    target = ['rating']\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "    # 2.count #unique features for each sparse field\n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features]\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    data_group = []\n",
    "    datas = []\n",
    "    for name, group in data.groupby('userId'):\n",
    "        data_group.append(group)\n",
    "    np.random.shuffle(data_group)\n",
    "    spilt_size = int(len(data_group)/FedCFG.num_clients)\n",
    "    for i in range(FedCFG.num_clients):\n",
    "        start = i*spilt_size\n",
    "        end = (i+1)*spilt_size\n",
    "        data_tem = pd.concat(data_group[start:end])\n",
    "        datas.append(data_tem)\n",
    "\n",
    "    return datas, feature_names, linear_feature_columns, dnn_feature_columns,  target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 联邦学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    a = [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "    return a\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.Client):\n",
    "    def __init__(self, cid, net, train_dataloader, val_dataset, val_dataloader, target):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.target = target\n",
    "\n",
    "    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:\n",
    "        ndarrays: List[np.ndarray] = get_parameters(self.net)\n",
    "        # Serialize ndarray's into a Parameters object\n",
    "        parameters = ndarrays_to_parameters(ndarrays)\n",
    "        # Build and return response\n",
    "        status = Status(code=Code.OK, message=\"Success\")\n",
    "        return GetParametersRes(\n",
    "            status=status,\n",
    "            parameters=parameters,\n",
    "        )\n",
    "\n",
    "    def fit(self, ins: FitIns) -> FitRes:\n",
    "        print(f\"[Client {self.cid}] fit, config: {ins.config}\")\n",
    "        # Deserialize parameters to NumPy ndarray's\n",
    "        parameters_original = ins.parameters\n",
    "        ndarrays_original = parameters_to_ndarrays(parameters_original)\n",
    "        # Update local model, train, get updated parameters\n",
    "        set_parameters(self.net, ndarrays_original)\n",
    "        self.net.fit(self.train_dataloader, self.train_dataloader[self.target].values,\n",
    "                     batch_size=256, epochs=10, verbose=2, validation_split=0.2)\n",
    "        ndarrays_updated = get_parameters(self.net)\n",
    "        # Serialize ndarray's into a Parameters object\n",
    "        parameters_updated = ndarrays_to_parameters(ndarrays_updated)\n",
    "        # Build and return response\n",
    "        status = Status(code=Code.OK, message=\"Success\")\n",
    "        return FitRes(\n",
    "            status=status,\n",
    "            parameters=parameters_updated,\n",
    "            num_examples=len(self.train_dataloader),\n",
    "            metrics={},\n",
    "        )\n",
    "\n",
    "    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {ins.config}\")\n",
    "\n",
    "        # Deserialize parameters to NumPy ndarray's\n",
    "        parameters_original = ins.parameters\n",
    "        ndarrays_original = parameters_to_ndarrays(parameters_original)\n",
    "\n",
    "        set_parameters(self.net, ndarrays_original)\n",
    "        pred_ans = self.net.predict(self.val_dataloader, batch_size=256)\n",
    "        mse = round(mean_squared_error(\n",
    "            self.val_dataset[self.target].values, pred_ans), 4)\n",
    "\n",
    "        # Build and return response\n",
    "        status = Status(code=Code.OK, message=\"Success\")\n",
    "        return EvaluateRes(\n",
    "            status=status,\n",
    "            loss=float(mse),\n",
    "            num_examples=len(self.val_dataloader),\n",
    "            metrics={\"mse\": float(mse)},\n",
    "        )\n",
    "\n",
    "\n",
    "# device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "# datas, feature_names, linear_feature_columns, dnn_feature_columns, target = get_datas()\n",
    "# train_datasets = [None for _ in range(FedCFG.num_clients)]\n",
    "# val_datasets = [None for _ in range(FedCFG.num_clients)]\n",
    "# train_dataloaders = [None for _ in range(FedCFG.num_clients)]\n",
    "# val_dataloaders = [None for _ in range(FedCFG.num_clients)]\n",
    "\n",
    "# for i in tqdm(range(FedCFG.num_clients)):\n",
    "#     train_datasets[i], val_datasets[i] = train_test_split(\n",
    "#         datas[i], test_size=0.2, random_state=i)\n",
    "#     train_dataloaders[i] = {name: train_datasets[i][name]\n",
    "#                             for name in feature_names}\n",
    "#     val_dataloaders[i] = {name: val_datasets[i][name]\n",
    "#                           for name in feature_names}\n",
    "\n",
    "\n",
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    # Load model\n",
    "    net = DeepFM(linear_feature_columns, dnn_feature_columns,\n",
    "                 task='regression', device=device)\n",
    "    net.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "    train_dataloader = train_dataloaders[int(cid)]\n",
    "    val_dataloader = val_dataloaders[int(cid)]\n",
    "    val_dataset = val_datasets[int(cid)]\n",
    "\n",
    "    return FlowerClient(cid, net, train_dataloader, val_dataset, val_dataloader, target)\n",
    "\n",
    "\n",
    "# strategy = fl.server.strategy.FedAvg(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=0.5,\n",
    "#     min_fit_clients=10,\n",
    "#     min_evaluate_clients=5,\n",
    "#     min_available_clients=10,\n",
    "# )\n",
    "\n",
    "# client_resources = None\n",
    "# if device.type == \"cuda\":\n",
    "#     client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# fl.simulation.start_simulation(\n",
    "#     client_fn=client_fn,\n",
    "#     num_clients=FedCFG.num_clients,\n",
    "#     config=fl.server.ServerConfig(num_rounds=5),\n",
    "#     strategy=strategy,\n",
    "#     client_resources=client_resources,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Train on 640133 samples, validate on 160034 samples, 2501 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:21, 117.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22s - loss:  0.9539 - mse:  0.9539 - val_mse:  0.8469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:21, 119.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "22s - loss:  0.8350 - mse:  0.8350 - val_mse:  0.8301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:22, 110.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "23s - loss:  0.8178 - mse:  0.8178 - val_mse:  0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:21, 118.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "22s - loss:  0.8032 - mse:  0.8032 - val_mse:  0.8099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:21, 116.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "22s - loss:  0.7889 - mse:  0.7889 - val_mse:  0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [00:21, 117.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "22s - loss:  0.7718 - mse:  0.7718 - val_mse:  0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1773it [00:14, 120.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/disk1/.Backup/movie_rec.ipynb 单元格 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m], )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# pickle.dump(model.state_dict(), open('model.pkl', 'wb'))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_model_input, train[target]\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225848333039307469227d/disk1/.Backup/movie_rec.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m pred_ans \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_model_input, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/deepctr_torch/models/basemodel.py:261\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle, callbacks)\u001b[0m\n\u001b[1;32m    259\u001b[0m loss_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    260\u001b[0m total_loss_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m--> 261\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    262\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data, feature_names, linear_feature_columns, dnn_feature_columns, target = get_data()\n",
    "pickle.dump(feature_names, open('feature_names.pkl', 'wb'))\n",
    "pickle.dump(linear_feature_columns, open('linear_feature_columns.pkl', 'wb'))\n",
    "pickle.dump(dnn_feature_columns, open('dnn_feature_columns.pkl', 'wb'))\n",
    "# 3.generate input data for model\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "# train_model_input['genres']=train['genres']\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "# test_model_input['genres']=test['genres']\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(512, 256, 128),\n",
    "               task='regression', device=device)\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "# pickle.dump(model.state_dict(), open('model.pkl', 'wb'))\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                    batch_size=256, epochs=20, verbose=1, validation_split=0.2)\n",
    "torch.save(model.state_dict(), 'model.pkl')\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "print(\"test MSE\", round(mean_squared_error(\n",
    "    test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
